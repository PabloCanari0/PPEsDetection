{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvZHHkd0oJeF"
   },
   "source": [
    "# **PPE DETECTION WITH MULTIPLE MODELS** *texto en cursiva*\n",
    "In this notebook, different deep learning models will be trained in order to achieve the detection of different personal protection equipment as well as some heavy machinery vehicles.\n",
    "\n",
    "Object detection includes categorising and locating objects from diverse categories or classes in an image.\n",
    "\n",
    "In this case, the different object classes to distinguish are:\n",
    "- mask\n",
    "- helmet\n",
    "- vest\n",
    "- boots\n",
    "- gloves\n",
    "- glasses\n",
    "- ear protection\n",
    "- human or person\n",
    "- bulldozer\n",
    "- dumb truck\n",
    "- excavator\n",
    "- road roller\n",
    "- wheel loader\n",
    "- background, empty site or null\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0YMOf2UxTLYb"
   },
   "source": [
    "## 2. Batching the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z9NFBU2eZMJ8",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# {   1   }\n",
    "import os\n",
    "import torch\n",
    "import string\n",
    "from collections import defaultdict\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "\n",
    "\n",
    "class PPEsDataset(Dataset):\n",
    "    # Personal Protection Equipment Dataset Class for defining different datasets\n",
    "\n",
    "  def __init__(self, csv_file, root_dir, augmentation_method=None, transform=None):\n",
    "    self.PPE_frame = pd.read_csv(csv_file)\n",
    "    self.root_dir = root_dir\n",
    "    self.transform = transform\n",
    "\n",
    "    # Save all the JPGs to check if they are present in the CSV file\n",
    "    all_files = set()\n",
    "    for root, dirs, files in os.walk(root_dir):\n",
    "      all_files.update(files)\n",
    "\n",
    "    # Preprocess annotations to avoid scanning the CSV every time\n",
    "    self.annotations = defaultdict(list)\n",
    "    for _, row in self.PPE_frame.iterrows():\n",
    "      image_name = row[0]\n",
    "      if image_name in all_files: # First check if the image is present in CSV file\n",
    "        bbox = row[4:8].values.astype(np.float32)\n",
    "        label = row[3]\n",
    "        self.annotations[image_name].append((bbox, label))\n",
    "\n",
    "    # Store list of unique image names that have annotations\n",
    "    all_image_names = self.PPE_frame.iloc[:, 0].unique()\n",
    "    self.image_list = [img for img in all_image_names if img in self.annotations]\n",
    "\n",
    "    # Category to integer mapping\n",
    "    self.category_map = {\n",
    "        'background': 0,\n",
    "        'vest': 1,\n",
    "        'helmet': 2,\n",
    "        'gloves': 3,\n",
    "        'glasses': 4,\n",
    "        'mask': 5,\n",
    "        'boots': 6,\n",
    "        'ear_protection': 7,\n",
    "        'human': 8,\n",
    "        'bulldozer': 9,\n",
    "        'dump_truck': 10,\n",
    "        'excavator': 11,\n",
    "        'road_roller': 12,\n",
    "        'wheel_loader': 13\n",
    "    }\n",
    "\n",
    "  def __len__(self): # Return CSV length\n",
    "    return len(self.image_list)\n",
    "\n",
    "  def __countCategory__(self): # Number of elements from each class\n",
    "    accElements = defaultdict(int) # Dictionary with default values\n",
    "    for _, row in self.PPE_frame.iterrows(): # For all the rows in the CSV file\n",
    "      category = row[3]  # Read category\n",
    "      accElements[category] += 1  # Increment the category counter\n",
    "    # Turn into a regular dictionary\n",
    "    accElements = dict(accElements)\n",
    "    return(accElements)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    if torch.is_tensor(idx):\n",
    "        idx = idx.tolist()  # Turn to a regular expresion\n",
    "\n",
    "    img_name = self.image_list[idx]\n",
    "    image_path = os.path.join(self.root_dir, img_name) # Full image path\n",
    "\n",
    "    # Load image and convert to RGB\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_np = np.array(image)\n",
    "\n",
    "    annotations = self.annotations[img_name] # Extract labels\n",
    "    bboxes = np.array([ann[0] for ann in annotations], dtype=np.float32) # Bounding boxes coordinates in array form\n",
    "    categories = [ann[1] for ann in annotations] # Labels\n",
    "\n",
    "    if self.transform: # Apply resize transform\n",
    "        transformed = self.transform(image=image_np, bboxes=bboxes, category=categories)\n",
    "        image_np = transformed['image']\n",
    "        bboxes = transformed['bboxes']\n",
    "        categories = transformed['category']\n",
    "\n",
    "    # Convert image to tensor and normalize\n",
    "    image_tensor = torch.tensor(image_np).permute(2, 0, 1).float() / 255.0 #Permute channels for correct visualization\n",
    "\n",
    "    numerical_categories = [self.category_map[cat] for cat in categories] # Convert category names to integers\n",
    "    categories_tensor = torch.tensor(numerical_categories, dtype=torch.int64) # From integer to tensor\n",
    "    bboxes_tensor = torch.tensor(bboxes, dtype=torch.float32) # Turn bboxes to integers also\n",
    "\n",
    "    image_id = torch.tensor([idx])     # Use integer ID for image_id (COCO format requires integer)\n",
    "\n",
    "    target = { # Full annotations object\n",
    "        'boxes': bboxes_tensor,\n",
    "        'labels': categories_tensor,\n",
    "        'image_id': image_id,\n",
    "    } \n",
    "\n",
    "    return image_tensor, target # Return image and annotations\n",
    "\n",
    "  def showDatasetImage(self, image, bboxes, categories):\n",
    "    index_to_category = {\n",
    "        0: 'background', 1: 'vest', 2: 'helmet', 3: 'gloves', 4: 'glasses', 5: 'mask',\n",
    "        6: 'boots', 7: 'ear_protection', 8: 'human', 9: 'bulldozer',\n",
    "        10: 'dump_truck', 11: 'excavator', 12: 'road_roller',\n",
    "        13: 'wheel_loader'\n",
    "    }\n",
    "    labels_text = [index_to_category[int(idx)] for idx in categories]  # Ensure categories are ints\n",
    "    image_tensor=image.as_subclass(torch.Tensor) # Get the underlying tensor and rescale\n",
    "    bboxes_tensor = torch.tensor(bboxes, dtype=torch.float32) # Convert bounding boxes to tensors\n",
    "    image_to_show=(image_tensor*255).type(torch.uint8) # Reorder channels for visualization (width, height, image)\n",
    "    image_with_boxes = draw_bounding_boxes(image_to_show, bboxes_tensor, labels=labels_text, colors=\"red\", width=2) # Draw image with boxes\n",
    "    plt.imshow(image_with_boxes.permute(1, 2, 0))  # Reorder channels for visualization (width, height, image)\n",
    "    plt.title('Dataset image')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    print(\"Categories:\", labels_text)\n",
    "\n",
    "\n",
    "  def Visualizator(self, index=None):\n",
    "    if index is None: # If there is no index specified\n",
    "        index = random.randint(0, len(self.image_list) - 1) # Use a random one\n",
    "    image, target = self[index] # Calls get item method\n",
    "    boxes = target['boxes']  # Should be tensor (N,4)\n",
    "    labels = target['labels']  # Should be tensor (N,)\n",
    "    self.showDatasetImage(image, boxes, labels)\n",
    "\n",
    "# Resize transform\n",
    "transformResize=A.Compose([A.Resize(height=640,width=640)], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['category']))\n",
    "\n",
    "def collate_func_train(batch): # For train dataloader\n",
    "    images, targets = zip(*batch)\n",
    "    new_targets = [{'boxes': t['boxes'], 'labels': t['labels']} for t in targets] #Turns labels into lists\n",
    "    return list(images), new_targets # Turn images into a list to get .item function\n",
    "\n",
    "def collate_func_test(batch):\n",
    "    images, targets = zip(*batch)\n",
    "    return list(images), list(targets) # Turn images and labels into a listo to get .item function\n",
    "\n",
    "# Define an element of the defined class for our dataset\n",
    "FinalDataset=PPEsDataset(csv_file=\"/kaggle/input/ppe-and-heavy-machinery-detection/FinalDataset/final_dataset_normalized.csv\",\n",
    "                          root_dir=\"/kaggle/input/ppe-and-heavy-machinery-detection/FinalDataset\",\n",
    "                         transform=None)\n",
    "\n",
    "FinalDataset.Visualizator() # Visualize an image from the set\n",
    "FinalDataset.__countCategory__() # Nº of instances per class\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-7bBuS6I5ey"
   },
   "source": [
    "The dataset can be split into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YeTJfXvPfg4L",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# {   2   }\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "def subset_split(dataset, valid_size=1000, seed=42):\n",
    "    #  Construct a map image → class\n",
    "    image_to_classes = {}\n",
    "    class_to_images = defaultdict(set)\n",
    "\n",
    "    for idx, img_name in enumerate(dataset.image_list): # Iterate over all images\n",
    "        anns = dataset.annotations[img_name] # Get image labels\n",
    "        class_labels = set([label for _, label in anns])\n",
    "        image_to_classes[img_name] = class_labels # For an image, get it´s labels\n",
    "        for label in class_labels:\n",
    "            class_to_images[label].add(img_name) # For a label add image\n",
    "\n",
    "    # Shuffle all the images\n",
    "    all_images = list(image_to_classes.keys())\n",
    "    random.seed(seed)\n",
    "    random.shuffle(all_images) # Mix all the images\n",
    "\n",
    "    selected_valid = set()\n",
    "    class_counts = defaultdict(int)\n",
    "\n",
    "    # Select validation images until you have the number of images you want\n",
    "    for img in all_images:\n",
    "        if len(selected_valid) >= valid_size:\n",
    "            break\n",
    "        if img in selected_valid: # Skip already included images\n",
    "            continue\n",
    "        selected_valid.add(img) # Add element to validation\n",
    "        for label in image_to_classes[img]: # Keep a count of labels in validation set\n",
    "            class_counts[label] += 1\n",
    "\n",
    "    # Split in training and validation set\n",
    "    val_indices = [i for i, img in enumerate(dataset.image_list) if img in selected_valid]\n",
    "    train_indices = [i for i, img in enumerate(dataset.image_list) if img not in selected_valid]\n",
    "\n",
    "    # Create the subsets\n",
    "    dataset_train = Subset(dataset, train_indices)\n",
    "    dataset_test = Subset(dataset, val_indices)\n",
    "\n",
    "    return dataset_train, dataset_test\n",
    "\n",
    "def count_classes_in_subset(dataset, indices): # Function to count classes in each set\n",
    "    accElements = defaultdict(int)\n",
    "    for idx in indices: # Go over all images\n",
    "        img_name = dataset.image_list[idx]\n",
    "        annotations = dataset.annotations[img_name]\n",
    "        for _, category in annotations: # Check category \n",
    "            accElements[category] += 1 # Add one corresponding unit\n",
    "    return dict(accElements)\n",
    "\n",
    "dataset_train, dataset_test = subset_split(FinalDataset, valid_size=5000, seed=42)\n",
    "train_files = set([dataset.image_list[i] for i in dataset_train.indices])\n",
    "test_files = set([dataset.image_list[i] for i in dataset_test.indices])\n",
    "counts_train = count_classes_in_subset(dataset_train.dataset, dataset_train.indices)\n",
    "counts_test = count_classes_in_subset(dataset_test.dataset, dataset_test.indices)\n",
    "print(counts_train)\n",
    "print(counts_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5TzFSjNE939w"
   },
   "source": [
    "Now that we have our dataset loaded, we can use Pytorch DataLoader to extract mini-batches from the set. Also, data will be shuffled and processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hgto5SAg-J3i",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# {   3   }\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=8, shuffle=True, num_workers=2, collate_fn=collate_func_train, pin_memory=True)\n",
    "\n",
    "batch = next(iter(dataloader_train))  # Iterate over the dataloader\n",
    "images, targets = batch  # Unpack the images and targets\n",
    "\n",
    "\n",
    "# If you need the batch size (number of images)\n",
    "batch_size = len(images)\n",
    "\n",
    "index_to_category = { # map int→string for draw_bounding_boxes\n",
    "    0: 'background',\n",
    "    1: 'vest',\n",
    "    2: 'helmet',\n",
    "    3: 'gloves',\n",
    "    4: 'glasses',\n",
    "    5: 'mask',\n",
    "    6: 'boots',\n",
    "    7: 'ear_protection',\n",
    "    8: 'human',\n",
    "    9: 'bulldozer',\n",
    "    10: 'dump_truck',\n",
    "    11: 'excavator',\n",
    "    12: 'road_roller',\n",
    "    13: 'wheel_loader'\n",
    "}\n",
    "\n",
    "if batch_size >= 9:\n",
    "    num_to_display = 9\n",
    "else:\n",
    "    num_to_display = batch_size\n",
    "\n",
    "for i in range(num_to_display):\n",
    "    # targets[i] is already a dictionary with 'boxes' and 'labels'\n",
    "    boxes = targets[i]['boxes']\n",
    "    labels = targets[i]['labels']\n",
    "    image_to_show = images[i] * 255 # Image rescale\n",
    "\n",
    "    # Check if boxes is not empty before stacking\n",
    "    boxes = boxes if boxes.numel() > 0 else torch.empty(0, 4)\n",
    "    labels = labels if labels.numel() > 0 else torch.empty(0, dtype=torch.int64)\n",
    "\n",
    "    label_texts = [index_to_category[int(idx)] for idx in labels]\n",
    "\n",
    "    image_with_boxes = draw_bounding_boxes(image_to_show.type(torch.uint8), boxes, labels=label_texts, colors=\"red\", width=2)\n",
    "\n",
    "    print(f'Categories {label_texts}')\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(image_with_boxes.permute(1, 2, 0))\n",
    "    plt.title(f'Batch image {i}')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t5IbAyVlM7N5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# {   4   }\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=8, shuffle=True, num_workers=2, collate_fn=collate_func_test, pin_memory=True)\n",
    "\n",
    "batch = next(iter(dataloader_test))  # Iterate over the dataloader\n",
    "images, targets = batch  # Unpack the images and targets\n",
    "\n",
    "\n",
    "# If you need the batch size (number of images)\n",
    "batch_size = len(images)\n",
    "\n",
    "index_to_category = { # map int→string for draw_bounding_boxes\n",
    "    0: 'background',\n",
    "    1: 'vest',\n",
    "    2: 'helmet',\n",
    "    3: 'gloves',\n",
    "    4: 'glasses',\n",
    "    5: 'mask',\n",
    "    6: 'boots',\n",
    "    7: 'ear_protection',\n",
    "    8: 'human',\n",
    "    9: 'bulldozer',\n",
    "    10: 'dump_truck',\n",
    "    11: 'excavator',\n",
    "    12: 'road_roller',\n",
    "    13: 'wheel_loader'\n",
    "}\n",
    "\n",
    "if batch_size >= 9:\n",
    "    num_to_display = 9\n",
    "else:\n",
    "    num_to_display = batch_size\n",
    "\n",
    "for i in range(num_to_display):\n",
    "    # targets[i] is already a dictionary with 'boxes' and 'labels'\n",
    "    boxes = targets[i]['boxes']\n",
    "    labels = targets[i]['labels']\n",
    "    image_to_show = images[i] * 255 # Rescale\n",
    "\n",
    "    # Check if boxes is not empty before stacking\n",
    "    boxes = boxes if boxes.numel() > 0 else torch.empty(0, 4)\n",
    "    labels = labels if labels.numel() > 0 else torch.empty(0, dtype=torch.int64)\n",
    "\n",
    "    label_texts = [index_to_category[int(idx)] for idx in labels]\n",
    "\n",
    "    image_with_boxes = draw_bounding_boxes(image_to_show.type(torch.uint8), boxes, labels=label_texts, colors=\"red\", width=2)\n",
    "\n",
    "    print(f'Categories {label_texts}')\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(image_with_boxes.permute(1, 2, 0))\n",
    "    plt.title(f'Batch image {i}')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9OWTPM0TMI_"
   },
   "source": [
    "Now that the available data can be accessed in batches, the different models can be trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wuIC0Db-TVJH"
   },
   "source": [
    "# 3. Training the models\n",
    "For the academic purpose of this project, the models that will be used are:\n",
    "\n",
    "\n",
    "*    Faster R-CNN\n",
    "*    FCOS\n",
    "*    RetinaNet\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-2_tzlTYgny"
   },
   "source": [
    "## 3.1 Faster R-CNN [2]\n",
    "This kind of approach can be divided into two steps:\n",
    "\n",
    "\n",
    "1.   Proposing regions where objects might be located\n",
    "2.   Proposing which class the object from that region is part of\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lp_bsT4ihmM5"
   },
   "source": [
    " A new backbone has to be added. The backbone of a Faster R-CNN model is the CNN which extracts important features from the images and configures a feature map. \"*mobilenet_v2*\" will be used as bakcbone. Anchors, which represent features in the feature map will be generated, also, the cropping of the interest regions in the feature map is configured. All this is put together into the final model, for the part of object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IB9kyuXYic1t",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# {   5   }\n",
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "# Redefine the number of classes, 14 in this case\n",
    "n_classes=14\n",
    "\n",
    "# load a pre-trained model for classification\n",
    "backbone = torchvision.models.mobilenet_v2(weights=\"DEFAULT\").features\n",
    "# ``FasterRCNN`` needs to know the number of\n",
    "# output channels in a backbone. For mobilenet_v2, it's 1280\n",
    "# so we need to add it here\n",
    "backbone.out_channels = 1280\n",
    "\n",
    "# let's make the RPN generate 5 x 3 anchors per spatial\n",
    "# location, with 5 different sizes and 3 different aspect\n",
    "# ratios. We have a Tuple[Tuple[int]] because each feature\n",
    "# map could potentially have different sizes and\n",
    "# aspect ratios\n",
    "anchor_generator = AnchorGenerator(\n",
    "    sizes=((32, 64, 128, 256, 512),),\n",
    "    aspect_ratios=((0.5, 1.0, 2.0),)\n",
    ")\n",
    "\n",
    "# let's define what are the feature maps that we will\n",
    "# use to perform the region of interest cropping, as well as\n",
    "# the size of the crop after rescaling.\n",
    "# if your backbone returns a Tensor, featmap_names is expected to\n",
    "# be [0]. More generally, the backbone should return an\n",
    "# ``OrderedDict[Tensor]``, and in ``featmap_names`` you can choose which\n",
    "# feature maps to use.\n",
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n",
    "    featmap_names=['0'],\n",
    "    output_size=7,\n",
    "    sampling_ratio=2\n",
    ")\n",
    "\n",
    "# put the pieces together inside a Faster-RCNN model\n",
    "model = FasterRCNN(\n",
    "    backbone,\n",
    "    num_classes=n_classes,\n",
    "    rpn_anchor_generator=anchor_generator,\n",
    "    box_roi_pool=roi_pooler\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVQP_YdumGMk"
   },
   "source": [
    "Everything is ready for training the model. But first, some helper functions to simplify training and evaluating detection models will be downloaded to make the work easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SHLwOdBEmcf_",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# {   6   }\n",
    "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\")\n",
    "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CE0-qVG812cj"
   },
   "source": [
    "We will define a training and evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KlSi801YeRhM",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install torchmetrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hrcFGsTL1or-",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# {   7   }\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from sklearn.metrics import f1_score, precision_recall_curve, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import utils\n",
    "\n",
    "# Training function\n",
    "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq, scaler=None):\n",
    "    model.train() # Train mode\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter(\"lr\", utils.SmoothedValue(window_size=1, fmt=\"{value:.6f}\"))\n",
    "    header = f\"Epoch: [{epoch}]\"\n",
    "\n",
    "    # To store loss values per batch\n",
    "    loss_history = []\n",
    "\n",
    "    lr_scheduler = None\n",
    "    if epoch == 0:\n",
    "        warmup_factor = 1.0 / 1000 # Warm up value for learning rate\n",
    "        warmup_iters = min(1000, len(data_loader) - 1)\n",
    "\n",
    "        lr_scheduler = torch.optim.lr_scheduler.LinearLR( # Linear learning rate warm up\n",
    "            optimizer, start_factor=warmup_factor, total_iters=warmup_iters\n",
    "        )\n",
    "\n",
    "    for step, (images, targets) in enumerate(metric_logger.log_every(data_loader, print_freq, header)): # For every image and labels in set (loaded from dataloader)\n",
    "        images = [image.to(device) for image in images] # Send images to GPU\n",
    "        targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets] # Send labels to GPU\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=scaler is not None): # Mixed precision training\n",
    "            loss_dict = model(images, targets) # Calculate losses (classifiaction, regression)\n",
    "            losses = sum(loss for loss in loss_dict.values()) # Total loss\n",
    "\n",
    "        # Reduce losses across all GPUs if using distributed training\n",
    "        loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
    "        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
    "\n",
    "        loss_value = losses_reduced.item()\n",
    "\n",
    "        if not math.isfinite(loss_value): # Verify if loss is a finite number\n",
    "            print(f\"Loss is {loss_value}, stopping training\")\n",
    "            print(loss_dict_reduced)\n",
    "            sys.exit(1)\n",
    "\n",
    "        optimizer.zero_grad() # Prepare to calculate parameters\n",
    "        if scaler is not None:\n",
    "            scaler.scale(losses).backward() # Bakcpropagation\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update() # Update weights\n",
    "        else: # For no scaler (not the case in this project)\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step() # Update learning rate\n",
    "\n",
    "        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "        # Save per-batch loss data\n",
    "        loss_history.append({\n",
    "            \"epoch\": epoch,\n",
    "            \"step\": step,\n",
    "            \"loss\": loss_value,\n",
    "            \"loss_classifier\": loss_dict_reduced[\"loss_classifier\"].item(),\n",
    "            \"loss_box_reg\": loss_dict_reduced[\"loss_box_reg\"].item(),\n",
    "            \"loss_objectness\": loss_dict_reduced[\"loss_objectness\"].item(),\n",
    "            \"loss_rpn_box_reg\": loss_dict_reduced[\"loss_rpn_box_reg\"].item()\n",
    "        })\n",
    "\n",
    "    # Save the loss history to a CSV file\n",
    "    loss_dir = \"/kaggle/working/loss_evo/frcnn\"\n",
    "    if not os.path.exists(loss_dir):\n",
    "      os.makedirs(loss_dir)\n",
    "    df = pd.DataFrame(loss_history)\n",
    "    df.to_csv(os.path.join(loss_dir, f\"loss_epoch_{epoch}.csv\"), index=False)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, dataloader, num_classes, score_threshold=0.5):\n",
    "    model.eval()\n",
    "\n",
    "    metric_map = MeanAveragePrecision(iou_type=\"bbox\")\n",
    "\n",
    "    all_preds = []\n",
    "    all_targs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in dataloader:\n",
    "            images = [img.to(device) for img in images] # Send images to GPU\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets] # Send labels to GPU\n",
    "\n",
    "            outputs = model(images) # Calculate predictions\n",
    "\n",
    "            batch_preds = []\n",
    "            batch_targs = []\n",
    "\n",
    "            for out, tgt in zip(outputs, targets): # For all predictions and labels (groundtruths)\n",
    "                batch_preds.append({\n",
    "                    \"boxes\": out[\"boxes\"].detach().cpu(),\n",
    "                    \"scores\": out[\"scores\"].detach().cpu(),\n",
    "                    \"labels\": out[\"labels\"].detach().cpu()\n",
    "                })\n",
    "                batch_targs.append({\n",
    "                    \"boxes\": tgt[\"boxes\"].detach().cpu(),\n",
    "                    \"labels\": tgt[\"labels\"].detach().cpu()\n",
    "                })\n",
    "\n",
    "            metric_map.update(batch_preds, batch_targs) # Store predictions and ground truths\n",
    "\n",
    "            all_preds.extend(batch_preds)\n",
    "            all_targs.extend(batch_targs)\n",
    "\n",
    "    map_result = metric_map.compute() # Calculate mAP from accumulated predictions and GTs\n",
    "    print(\"mAP results:\", map_result)\n",
    "\n",
    "    pr_scores = []\n",
    "    pr_tp_flags = []\n",
    "\n",
    "    matched_true_filtered = []\n",
    "    matched_pred_filtered = []\n",
    "\n",
    "    for out, tgt in zip(all_preds, all_targs): #For all predictions and all targets\n",
    "        pred_boxes = out[\"boxes\"] # Predicted bboxes\n",
    "        pred_labels = out[\"labels\"] # Predicted labels\n",
    "        pred_scores = out[\"scores\"] # Prediction score\n",
    "        gt_boxes = tgt[\"boxes\"] # Ground truth bboxes\n",
    "        gt_labels = tgt[\"labels\"] # Ground truth labels\n",
    "\n",
    "        if len(gt_boxes) == 0: # Image has no labels\n",
    "            # No GT: all preds are false positives for PR\n",
    "            for i in range(len(pred_boxes)):\n",
    "                pr_tp_flags.append(0)\n",
    "                pr_scores.append(pred_scores[i].item())\n",
    "            # For F1/confusion: filter by score and mark all as FP with background GT\n",
    "            for i in range(len(pred_boxes)):\n",
    "                if pred_scores[i] >= score_threshold:\n",
    "                    matched_true_filtered.append(0)  # Background class\n",
    "                    matched_pred_filtered.append(pred_labels[i].item())\n",
    "            continue\n",
    "\n",
    "        if len(pred_boxes) == 0: # No predictions (background)\n",
    "            # No predictions: all GT are false negatives for F1/confusion\n",
    "            for j in range(len(gt_boxes)):\n",
    "                matched_true_filtered.append(gt_labels[j].item())\n",
    "                matched_pred_filtered.append(0)  # Background prediction \n",
    "            # No preds for PR curve here\n",
    "            continue\n",
    "\n",
    "        ious = torchvision.ops.box_iou(pred_boxes, gt_boxes) #Calculate IoU for each pair of prediction-ground-truth\n",
    "\n",
    "        gt_matched_pr = torch.zeros(len(gt_boxes))  # For PR matching (all preds)\n",
    "        gt_matched_f1 = torch.zeros(len(gt_boxes))  # For F1/confusion matching (filtered preds)\n",
    "\n",
    "        # reorder scores in descendent order\n",
    "        sorted_indices = torch.argsort(pred_scores, descending=True)\n",
    "        pred_boxes = pred_boxes[sorted_indices]\n",
    "        pred_labels = pred_labels[sorted_indices]\n",
    "        pred_scores = pred_scores[sorted_indices]\n",
    "        ious = ious[sorted_indices]\n",
    "        \n",
    "        for i in range(len(pred_boxes)): # For all predictions\n",
    "            iou_row = ious[i] # Get the IoU  for the prediction with each GT\n",
    "            max_iou, max_j = iou_row.max(0) # Get the best match according to IoU\n",
    "\n",
    "            # Precision-Recall curve calculation (all preds, no score filtering)\n",
    "            if max_iou >= 0.3: # The IoU surpasses 0.3?\n",
    "                if pred_labels[i] == gt_labels[max_j] and gt_matched_pr[max_j] == 0: # Box isn't macthed and class is correct\n",
    "                    pr_tp_flags.append(1) #TP \n",
    "                    gt_matched_pr[max_j] = 1 # GT matched\n",
    "                else:\n",
    "                    pr_tp_flags.append(0) #FP (GT not matched)\n",
    "            else:\n",
    "                pr_tp_flags.append(0) #FP (GT not matched)\n",
    "            \n",
    "            pr_scores.append(pred_scores[i].item()) # Get prediction score\n",
    "\n",
    "            # F1 and confusion matrix calculation (filtered by score)\n",
    "            if pred_scores[i] >= score_threshold: # Score > 0.5\n",
    "                if max_iou>=0.3: # Match by box location\n",
    "                    if pred_labels[i] == gt_labels[max_j] and gt_matched_f1[max_j] == 0: #TP\n",
    "                        matched_true_filtered.append(gt_labels[max_j].item()) # GT\n",
    "                        matched_pred_filtered.append(pred_labels[i].item()) # Predicted class\n",
    "                        gt_matched_f1[max_j] = 1  # Mark GT matched for F1/confusion\n",
    "                    elif gt_matched_f1[max_j] == 0 :  #FP for wrong class\n",
    "                        # Localization match but wrong class (classification FP)\n",
    "                        matched_true_filtered.append(gt_labels[max_j].item()) # GT \n",
    "                        matched_pred_filtered.append(pred_labels[i].item())# Predicted class\n",
    "                    elif pred_labels[i] == gt_labels[max_j]: # FP for already paired GT (overprediction)\n",
    "                        matched_true_filtered.append(0) # Background \n",
    "                        matched_pred_filtered.append(pred_labels[i].item()) # Predicted class\n",
    "                else:\n",
    "                    # No localization match (localization FP)\n",
    "                    matched_true_filtered.append(0) # Background\n",
    "                    matched_pred_filtered.append(pred_labels[i].item()) # Predicted class\n",
    "\n",
    "        # Add False Negatives (GT not matched in F1/confusion)\n",
    "        for j in range(len(gt_boxes)):\n",
    "            if gt_matched_f1[j] == 0: # Unpaired GT\n",
    "                matched_true_filtered.append(gt_labels[j].item()) # GT\n",
    "                matched_pred_filtered.append(0) # Bakcground predicted\n",
    "\n",
    "    labels_range = list(range(num_classes))\n",
    "\n",
    "    f1_per_class = f1_score(matched_true_filtered, matched_pred_filtered, labels=labels_range, average=None, zero_division=0) # Calculate F1 per class\n",
    "    f1_macro = f1_score(matched_true_filtered, matched_pred_filtered, labels=labels_range, average='macro', zero_division=0) # General F1\n",
    "    precision, recall, thresholds = precision_recall_curve(pr_tp_flags, pr_scores) # Calculate precision and recall\n",
    "\n",
    "    print(\"F1-score per class:\", f1_per_class)\n",
    "    print(\"Macro F1-score:\", f1_macro)\n",
    "\n",
    "    class_map = {\n",
    "        'background': 0,\n",
    "        'vest': 1,\n",
    "        'helmet': 2,\n",
    "        'gloves': 3,\n",
    "        'glasses': 4,\n",
    "        'mask': 5,\n",
    "        'boots': 6,\n",
    "        'ear_protection': 7,\n",
    "        'human': 8,\n",
    "        'bulldozer': 9,\n",
    "        'dump_truck': 10,\n",
    "        'excavator': 11,\n",
    "        'road_roller': 12,\n",
    "        'wheel_loader': 13\n",
    "    }\n",
    "    class_names = [k for k, v in sorted(class_map.items(), key=lambda item: item[1])]\n",
    "\n",
    "    cm = confusion_matrix(matched_true_filtered, matched_pred_filtered, labels=labels_range) # Confusion matrix\n",
    "\n",
    "    # Plot F1 score per class\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(class_names, f1_per_class, color='steelblue')\n",
    "    plt.xlabel(\"Classes\")\n",
    "    plt.ylabel(\"F1 Score\")\n",
    "    plt.title(\"F1 Score per Class\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Precision-Recall curve\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(recall, precision, label=\"Precision-Recall Curve\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Precision vs Recall Curve (Object Detection)\")\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Confusion Matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names, cmap=\"Blues\")\n",
    "    plt.xlabel(\"Predicted Class\")\n",
    "    plt.ylabel(\"True Class\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return map_result[\"map\"].item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2CKkNsMqHxca"
   },
   "source": [
    "A main function is written to train the model and evaluate every now and then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C9hRVdDwbc3O",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.environ['PYDEVD_DISABLE_FILE_VALIDATION'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "645DRbgvH19D",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# {   8   }\n",
    "#from engine import train_one_epoch, evaluate\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import  GradScaler\n",
    "\n",
    "# train on the GPU or on the CPU, if a GPU is not available\n",
    "if torch.cuda.is_available():\n",
    "  print(\"GPU available\")\n",
    "  device = torch.device('cuda')\n",
    "else:\n",
    "  print(\"GPU not available... Using CPU instead\")\n",
    "  device = torch.device('cpu')\n",
    "\n",
    "# Move model to GPU or CPU\n",
    "model.to(device) \n",
    "\n",
    "# Use a scaler for mixed precision training\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Set the optimizer (stochastic gradient descent)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Learning rate scheduler, modifies dinamically the lr throughout the training\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "# Number of training epocs\n",
    "num_epochs=10\n",
    "best_map=None\n",
    "n_epochs_stop=1 # If there are no improvements in this amount of epochs, stop the training\n",
    "evaluate_between=False\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, optimizer, dataloader_train, device, epoch, print_freq=10, scaler=scaler)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    if evaluate_between==True: # Choose wether to train inside the training loop or not\n",
    "      if (epoch+1) % 2 == 0 or epoch == num_epochs - 1: # Evaluate at epochs: 1,3,5,7...\n",
    "          # evaluate on the test dataset\n",
    "          current_map = evaluate(model, dataloader_test, n_classes)  # returns mAP, F1, recall-precision curve and confusion matrix\n",
    "          if best_map is None or current_map > best_map: # If results imporve\n",
    "              best_map = current_map\n",
    "              epochs_no_improve = 0\n",
    "          else:\n",
    "              epochs_no_improve += 1\n",
    "          if epochs_no_improve >= n_epochs_stop: # If too many epochs without improvement\n",
    "              print(f\"Early stopping in epoch {epoch} mAP is not improving\")\n",
    "              break\n",
    "\n",
    "\n",
    "# Save the fine tuned Faster R-CNN model\n",
    "# Create directory if it doesn't exist\n",
    "models_dir = \"/kaggle/working/trained_models\"\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "# Full path with filename to save the model\n",
    "model_path = os.path.join(models_dir, \"fasterrcnn_model.pth\")\n",
    "\n",
    "# Save the model state dictionary to the specified path\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "print(f\"Model saved at {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c979wUegvxeU"
   },
   "source": [
    "We can observe the loss evolution throughout the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lXIKuFFvv1W9",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# {   9   }\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt # gegbg\n",
    "\n",
    "\n",
    "# Path to the folder containing CSVs\n",
    "loss_dir = \"/kaggle/working/loss_evo/frcnn\"\n",
    "output_dir = os.path.join(loss_dir, \"loss_plots\")\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# List and sort the CSV files by epoch number\n",
    "csv_files = sorted(\n",
    "    [f for f in os.listdir(loss_dir) if f.startswith(\"loss_epoch_\") and f.endswith(\".csv\")],\n",
    "    key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0])\n",
    ")\n",
    "\n",
    "# Concatenate all DataFrames, keeping a global step index\n",
    "all_dfs = []\n",
    "global_step = 0\n",
    "\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(os.path.join(loss_dir, file))\n",
    "    df[\"global_step\"] = df.index + global_step\n",
    "    global_step = df[\"global_step\"].iloc[-1] + 1  # update global step count\n",
    "    all_dfs.append(df)\n",
    "\n",
    "# Merge all epochs into one DataFrame\n",
    "full_df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "# List of loss components to plot\n",
    "loss_components = [\"loss\", \"loss_classifier\", \"loss_box_reg\", \"loss_objectness\", \"loss_rpn_box_reg\"]\n",
    "titles = {\n",
    "    \"loss\": \"Total Loss\",\n",
    "    \"loss_classifier\": \"Classifier Loss\",\n",
    "    \"loss_box_reg\": \"Box Regression Loss\",\n",
    "    \"loss_objectness\": \"Objectness Loss\",\n",
    "    \"loss_rpn_box_reg\": \"RPN Box Regression Loss\"\n",
    "}\n",
    "\n",
    "# Plot each loss on a separate figure and save it\n",
    "for loss_name in loss_components:\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(full_df[\"global_step\"], full_df[loss_name], label=titles[loss_name], linewidth=2)\n",
    "    plt.xlabel(\"Global Step (accumulated batch index)\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Evolution of {titles[loss_name]}\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save figure\n",
    "    output_path = os.path.join(output_dir, f\"{loss_name}_evolution.png\")\n",
    "    plt.savefig(output_path, dpi=300)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7IJHgaXubLnk"
   },
   "source": [
    "Now that the model has been trained it is time to test it' average precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_dIzk3PvbSYu",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# {   10   }\n",
    "# Evaluation of the Faster R-CNN model\n",
    "import torch\n",
    "\n",
    "# Use GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load pretrained model\n",
    "state_dict=torch.load(\"/kaggle/working/trained_models/fasterrcnn_model.pth\")\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# Evaluate the model\n",
    "mAP_map=evaluate(model, dataloader_test, num_classes=n_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ann5L4MSUh1-"
   },
   "source": [
    "Let's try the model with a real picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gWqyBE63UklA",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# {   11  }\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "from google.colab.patches import cv2_imshow\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Use GPU\n",
    "\n",
    "img_dir = \"/kaggle/input/ppe-and-heavy-machinery-detection/FinalDataset\"\n",
    "imgs = [f for f in os.listdir(img_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "img_name = random.choice(imgs) # Get random image\n",
    "img_path = os.path.join(img_dir, img_name) # Get full path to image\n",
    "\n",
    "img = Image.open(img_path).convert(\"RGB\") # Opne image in RGB format\n",
    "\n",
    "transform = T.Compose([ # Rescale\n",
    "    T.Resize((640, 640)),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "img_tensor = transform(img).unsqueeze(0).to(device) # Turn into tensro and apply resize\n",
    "\n",
    "model.eval() # Evaluation mode\n",
    "with torch.no_grad():\n",
    "    preds = model(img_tensor) # predict objects in image\n",
    "\n",
    "boxes = preds[0]['boxes'].cpu().numpy()\n",
    "labels = preds[0]['labels'].cpu().numpy()\n",
    "scores = preds[0]['scores'].cpu().numpy()\n",
    "\n",
    "class_names = ['background','vest', 'helmet', 'gloves', 'glasses', 'mask', 'boots',\n",
    "               'ear_protection', 'human', 'bulldozer', 'dump_truck',\n",
    "               'excavator', 'road_roller', 'wheel_loader']\n",
    "\n",
    "img_cv = cv2.cvtColor(np.array(img.resize((640, 640))), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "score_threshold = 0.5  # Filter low confidence predictions\n",
    "detected_classes = set()\n",
    "\n",
    "for box, label, score in zip(boxes, labels, scores):\n",
    "    if score < score_threshold: # Filter predictions with low score\n",
    "        continue\n",
    "    detected_classes.add(class_names[label]) # Add label\n",
    "    x1, y1, x2, y2 = box.astype(int) # Bounding box\n",
    "    cv2.rectangle(img_cv, (x1, y1), (x2, y2), (0, 255, 0), 2) # Draw recatngle with bounding box coordinate\n",
    "    text = f\"{class_names[label]}: {score:.2f}\"  # Label text \n",
    "    cv2.putText(img_cv, text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, # Box format (green color)\n",
    "                0.5, (0, 255, 0), 2)\n",
    "\n",
    "print(\"Clases detectadas:\", \", \".join(sorted(detected_classes)))\n",
    "\n",
    "cv2_imshow(img_cv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUUxbfDIVmmA"
   },
   "source": [
    "# YOLO\n",
    "The next model we wil use is YOLO. The labels of the images now need to be in JSON format, therefore we need to make a format change.\n",
    "YOLO expects a YAML file, indicating paths to directories where images and labels in txt format are stored. The txt files will contain class_index x_center y_center width height. The name of the txt must match the corresponding image file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yVWF-SUWb0nU",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# {   12   }\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "def convert_csv_to_yolo(csv_path, images_dir, output_dir, train_files, test_files):\n",
    "    # Category map\n",
    "    category_map = {\n",
    "        'background': 0,\n",
    "        'vest': 1,\n",
    "        'helmet': 2,\n",
    "        'gloves': 3,\n",
    "        'glasses': 4,\n",
    "        'mask': 5,\n",
    "        'boots': 6,\n",
    "        'ear_protection': 7,\n",
    "        'human': 8,\n",
    "        'bulldozer': 9,\n",
    "        'dump_truck': 10,\n",
    "        'excavator': 11,\n",
    "        'road_roller': 12,\n",
    "        'wheel_loader': 13,\n",
    "    }\n",
    "\n",
    "    # Load annotations from CSV\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Create YOLO output directories\n",
    "    train_dir = os.path.join(output_dir, \"labels/train\")\n",
    "    test_dir = os.path.join(output_dir, \"labels/test\")\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "    # Inicializar contadores\n",
    "    train_class_counts = Counter()\n",
    "    test_class_counts = Counter()\n",
    "\n",
    "    all_files = train_files.union(test_files)\n",
    "\n",
    "    for filename in all_files:\n",
    "        image_path = os.path.join(images_dir, filename)\n",
    "\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"Image not found -> {filename}, ignored.\")\n",
    "            continue\n",
    "\n",
    "        subset = \"train\" if filename in train_files else \"test\"\n",
    "        yolo_lines = []\n",
    "\n",
    "        for _, row in df[df['filename'] == filename].iterrows():\n",
    "            xmin, ymin, xmax, ymax = row['xmin'], row['ymin'], row['xmax'], row['ymax']\n",
    "            category = row['class']\n",
    "            width, height = row['width'], row['height']\n",
    "\n",
    "            if category not in category_map:\n",
    "                print(f\"Warning: unknown label '{category}' en {filename}\")\n",
    "                continue\n",
    "\n",
    "            class_id = category_map[category]\n",
    "\n",
    "            if subset == \"train\":\n",
    "                train_class_counts[category] += 1\n",
    "            else:\n",
    "                test_class_counts[category] += 1\n",
    "\n",
    "            x_center = ((xmin + xmax) / 2) / width\n",
    "            y_center = ((ymin + ymax) / 2) / height\n",
    "            w = (xmax - xmin) / width\n",
    "            h = (ymax - ymin) / height\n",
    "\n",
    "            yolo_lines.append(f\"{class_id} {x_center:.6f} {y_center:.6f} {w:.6f} {h:.6f}\")\n",
    "\n",
    "        # Save .txt file\n",
    "        txt_filename = os.path.splitext(filename)[0] + \".txt\"\n",
    "        subdir = train_dir if subset == \"train\" else test_dir\n",
    "        txt_path = os.path.join(subdir, txt_filename)\n",
    "\n",
    "        with open(txt_path, 'w') as f:\n",
    "            f.write(\"\\n\".join(yolo_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g-4GW027JIEm",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# {   13   }\n",
    "convert_csv_to_yolo(csv_path=\"/kaggle/input/ppe-and-heavy-machinery-detection/FinalDataset/final_dataset_normalized.csv\",\n",
    "                    images_dir=\"/kaggle/input/ppe-and-heavy-machinery-detection/FinalDataset/\",\n",
    "                    output_dir=\"/kaggle/working/YOLO/\",\n",
    "                    train_files=train_files, test_files=test_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NVwud4yMT9BU"
   },
   "source": [
    "Let's move also the images to the YOLO directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L43_jX3TUCPE",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# {   14   }\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Paths - change these as needed\n",
    "original_images_dir = \"/kaggle/input/ppe-and-heavy-machinery-detection/FinalDataset\"\n",
    "train_txt_dir = \"/kaggle/working/YOLO/labels/train/\"\n",
    "test_txt_dir = \"/kaggle/working/YOLO/labels/test/\"\n",
    "target_images_train_dir = \"/kaggle/working/YOLO/images/train/\"\n",
    "target_images_test_dir = \"/kaggle/working/YOLO/images/test/\"\n",
    "\n",
    "# Create target directories if they don't exist\n",
    "os.makedirs(target_images_train_dir, exist_ok=True)\n",
    "os.makedirs(target_images_test_dir, exist_ok=True)\n",
    "\n",
    "# Valid image extensions (optional filter)\n",
    "valid_extensions = {'.jpg', '.jpeg', '.png'}\n",
    "\n",
    "# Iterate through all files in the original images directory\n",
    "for img_file in os.listdir(original_images_dir):\n",
    "    # Check if the file is an image\n",
    "    ext = os.path.splitext(img_file)[1].lower()\n",
    "    if ext not in valid_extensions:\n",
    "        continue  # Skip non-image files\n",
    "\n",
    "    # Derive the base filename without extension to find corresponding .txt\n",
    "    base_name = os.path.splitext(img_file)[0]\n",
    "    train_txt_path = os.path.join(train_txt_dir, base_name + \".txt\")\n",
    "    test_txt_path = os.path.join(test_txt_dir, base_name + \".txt\")\n",
    "\n",
    "    # Determine where the annotation file exists and copy the image accordingly\n",
    "    if os.path.exists(train_txt_path):\n",
    "        shutil.copy(os.path.join(original_images_dir, img_file), target_images_train_dir)\n",
    "        print(f\"Copied {img_file} to train images folder.\")\n",
    "    elif os.path.exists(test_txt_path):\n",
    "        shutil.copy(os.path.join(original_images_dir, img_file), target_images_test_dir)\n",
    "        print(f\"Copied {img_file} to test images folder.\")\n",
    "    else:\n",
    "        # If no corresponding txt annotation found, skip or handle as needed\n",
    "        print(f\"No annotation found for {img_file}, skipping.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-KEWSlQS2Ta"
   },
   "source": [
    "A YAML file needs o be conformed in order to train the YOLO model. Once all the preprocessing is done, we must train a specific YOLO model to train. YOLOv8 seems to be the most academic approach. Before training we will install the needed modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DqI-Vm2KbjOw",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y4wXs9Tjbnli",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "pip install clearml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OJaztsWQfjOz",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# {   15   }\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.yolo.engine.callback import Callback\n",
    "\n",
    "class BatchLossLogger(Callback):\n",
    "    def __init__(self, log_file=\"batch_loss_log.csv\"):\n",
    "        super().__init__()\n",
    "        self.log_file = log_file\n",
    "        # First line of loss file\n",
    "        with open(self.log_file, 'w') as f:\n",
    "            f.write(\"epoch,step,loss_total,loss_obj,loss_cls,loss_box\\n\")\n",
    "    \n",
    "    def on_train_batch_end(self, trainer, batch, outputs, batch_loss, **kwargs):\n",
    "        # This method is called after every training step (batch).\n",
    "        # trainer: contains training state\n",
    "        # batch: data from the current batch.\n",
    "        # outputs: model loss.\n",
    "        # batch_loss: batch loss.\n",
    "\n",
    "        # Obtain training state info\n",
    "        epoch = trainer.epoch  # current epoch\n",
    "        step = trainer.iter    # batch counter\n",
    "\n",
    "        # Total loss\n",
    "        loss_total = batch_loss.item() if isinstance(batch_loss, torch.Tensor) else batch_loss\n",
    "\n",
    "        # Extract sub losses.\n",
    "        loss_obj = outputs.get(\"loss_obj\", 0.0) if isinstance(outputs, dict) else 0.0\n",
    "        loss_cls = outputs.get(\"loss_cls\", 0.0) if isinstance(outputs, dict) else 0.0\n",
    "        loss_box = outputs.get(\"loss_box\", 0.0) if isinstance(outputs, dict) else 0.0\n",
    "\n",
    "        # Save to CSV file\n",
    "        with open(self.log_file, 'a') as f:\n",
    "            f.write(f\"{epoch},{step},{loss_total},{loss_obj},{loss_cls},{loss_box}\\n\")\n",
    "\n",
    "# Load the model\n",
    "model=YOLO('yolov8n.pt')\n",
    "\n",
    "# Callback to store loss throughout the training\n",
    "callbacks = [BatchLossLogger(log_file=\"/kaggle/working/batch_loss_log.csv\")]\n",
    "\n",
    "# Train the model\n",
    "results=model.train(data='/kaggle/input/YOLO_format.yaml', imgsz=640, epochs=10, batch=8, name='yolov8_model_train', callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the training loss evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {   16   }\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Load the CSV log file\n",
    "log_file = \"/kaggle/working/batch_loss_log.csv\"  # Asegúrate de que esta ruta es la correcta\n",
    "df = pd.read_csv(log_file)\n",
    "\n",
    "# Plot loss_box (regression loss)\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(df['step'], df['loss_box'], label='Box Regression Loss')\n",
    "plt.xlabel('Global Step (accumulated batch index)')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Evolution of Box Regression Loss')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot loss_cls (classification loss)\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(df['step'], df['loss_cls'], label='Classification Loss')\n",
    "plt.xlabel('Global Step (accumulated batch index)')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Evolution of Classifier Loss')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot total loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(df['step'], df['loss_total'], label='Total Loss')\n",
    "plt.xlabel('Global Step (accumulated batch index)')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Evolution of Total Loss')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let`s try the model with some actual images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# {   17   }\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "import os, random\n",
    "model = YOLO('/kaggle/working/runs/detect/yolov8_model_20/weights/best.pt')\n",
    "dir='/kaggle/working/YOLO/images/test'\n",
    "results = model.predict(source=os.path.join(dir,random.choice(os.listdir(dir))), save=False)\n",
    "\n",
    "for r in results:\n",
    "    im_array = r.plot()  # Image with boxes\n",
    "    im_rgb = cv2.cvtColor(im_array, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(im_rgb)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMPav4EzfSul"
   },
   "source": [
    "# RetinaNet\n",
    "For this final model, we count with a very special characteristic, which is Focal Loss, that allows us to act against class unbalance during training [5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XLRfuJaFYspS",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# {   18   }\n",
    "from functools import partial\n",
    "import torchvision\n",
    "from torchvision.models.detection.retinanet import RetinaNet\n",
    "from torchvision.models.detection.retinanet import RetinaNetClassificationHead\n",
    "from torchvision.models.detection.retinanet import RetinaNet_ResNet50_FPN_V2_Weights\n",
    "\n",
    "# Redefine the number of classes, 15 in this case\n",
    "n_classes=14\n",
    "\n",
    "# Constructs Retina-Net model with ResNet-50-FPN backbone and pretrained weights\n",
    "model=torchvision.models.detection.retinanet_resnet50_fpn_v2(weights=RetinaNet_ResNet50_FPN_V2_Weights.COCO_V1)\n",
    "\n",
    "# Define number of anchors. For ResNet-50FPN it's 9 (3 anchor sizes x 3 aspect ratios)\n",
    "num_anchors = model.head.classification_head.num_anchors\n",
    "\n",
    "# Edit the classifiaction head for our purpose\n",
    "model.head.classification_head=RetinaNetClassificationHead(\n",
    "    in_channels=256, # Input channels into classification head, 256 follows the RetinaNet architecture\n",
    "    num_anchors=num_anchors,\n",
    "    num_classes=n_classes,\n",
    "    norm_layer=partial(torch.nn.GroupNorm, 32) # Normalization layer after each convolution\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7I1wg2Iqo0tL"
   },
   "source": [
    "Most of the previous training function can be reused to train the RetinaNet model. The only difference is that now, there is no region proposal network, as RetinaNet is a one stage detection method. Therfore, we must eliminate the parts where the loss associated to this feature is calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9UwqWU3BqDAx",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# {   19   }\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import utils\n",
    "\n",
    "\n",
    "def train_one_epoch_RetinaNet(model, optimizer, data_loader, device, epoch, print_freq, scaler=None):\n",
    "    model.train() # Train mode\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter(\"lr\", utils.SmoothedValue(window_size=1, fmt=\"{value:.6f}\"))\n",
    "    header = f\"Epoch: [{epoch}]\"\n",
    "\n",
    "    # To store loss values per batch\n",
    "    loss_history = []\n",
    "\n",
    "    lr_scheduler = None\n",
    "    if epoch == 0:\n",
    "        warmup_factor = 1.0 / 1000\n",
    "        warmup_iters = min(1000, len(data_loader) - 1)\n",
    "\n",
    "        lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "            optimizer, start_factor=warmup_factor, total_iters=warmup_iters # Linear learning rate warm up\n",
    "        )\n",
    "\n",
    "    for step, (images, targets) in enumerate(metric_logger.log_every(data_loader, print_freq, header)): # For each image and it´s labels\n",
    "        images = [image.to(device) for image in images] # Send images to GPU\n",
    "        targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets] # Send labels to GPU\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=scaler is not None): # Mixed precision training\n",
    "            loss_dict = model(images, targets) # Calculate losses made by the model\n",
    "            losses = sum(loss for loss in loss_dict.values()) # Total loss\n",
    "\n",
    "        # Reduce losses across all GPUs if using distributed training\n",
    "        loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
    "        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
    "\n",
    "        loss_value = losses_reduced.item()\n",
    "\n",
    "        if not math.isfinite(loss_value): # Check if loss is a finite value\n",
    "            print(f\"Loss is {loss_value}, stopping training\")\n",
    "            print(loss_dict_reduced)\n",
    "            sys.exit(1)\n",
    "\n",
    "        optimizer.zero_grad() # Prepare to calculate parameters\n",
    "        if scaler is not None:\n",
    "            scaler.scale(losses).backward() # Backpropagation \n",
    "            scaler.step(optimizer)\n",
    "            scaler.update() # Update weights\n",
    "        else:\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "        # Save per-batch loss data\n",
    "        loss_history.append({\n",
    "          \"epoch\": epoch,\n",
    "          \"step\": step,\n",
    "          \"loss\": loss_value,\n",
    "          \"classification\": loss_dict_reduced.get(\"classification\", torch.tensor(0.0)).item(),\n",
    "          \"bbox_regression\": loss_dict_reduced.get(\"bbox_regression\", torch.tensor(0.0)).item(),\n",
    "        })\n",
    "\n",
    "    # Save the loss history to a CSV file\n",
    "    loss_dir = \"/kaggle/working/loss_evo/frcnn\"\n",
    "    if not os.path.exists(loss_dir):\n",
    "      os.makedirs(loss_dir)\n",
    "    df = pd.DataFrame(loss_history)\n",
    "    df.to_csv(os.path.join(loss_dir, f\"loss_epoch_{epoch}.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZU473JiRqVi9"
   },
   "source": [
    "Again, a main loop for the training is coded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "645DRbgvH19D",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# {   20   }\n",
    "#from engine import train_one_epoch, evaluate\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import  GradScaler\n",
    "\n",
    "# train on the GPU or on the CPU, if a GPU is not available\n",
    "if torch.cuda.is_available():\n",
    "  print(\"GPU available\")\n",
    "  device = torch.device('cuda')\n",
    "else:\n",
    "  print(\"GPU not available... Using CPU instead\")\n",
    "  device = torch.device('cpu')\n",
    "\n",
    "# Move model to GPU or CPU\n",
    "model.to(device)\n",
    "\n",
    "# Use a scaler for mixed precision training\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Set the optimizer (stochastic gradient descent)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Learning rate scheduler, modifies dinamically the lr throughout the training\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.25)\n",
    "\n",
    "# Number of training epocs\n",
    "num_epochs=20\n",
    "best_map=None\n",
    "n_epochs_stop=1 # If there are no improvements in this amount of epochs, stop the training\n",
    "evaluate_between=False\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch_RetinaNet(model, optimizer, dataloader_train, device, epoch, print_freq=10, scaler=scaler)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    if evaluate_between==True: # Choose wether to train inside the training loop or not\n",
    "      if (epoch+1) % 2 == 0 or epoch == num_epochs - 1: # Evaluate in epochs 1,3,5,7,9...\n",
    "          # evaluate on the test dataset\n",
    "          current_map = evaluate(model, dataloader_test, n_classes)  # returns mAP, F1 score, recall vs precision curve, confusion matrix\n",
    "          if best_map is None or current_map > best_map: # If evaluation results show improvement\n",
    "              best_map = current_map\n",
    "              epochs_no_improve = 0\n",
    "          else:\n",
    "              epochs_no_improve += 1  # If not, count epochs without improving\n",
    "          if epochs_no_improve >= n_epochs_stop: # Too many epochs without improving\n",
    "              print(f\"Early stopping in epoch {epoch} mAP is not improving\")\n",
    "              break\n",
    "\n",
    "\n",
    "# Save the fine tuned Faster R-CNN model\n",
    "# Create directory if it doesn't exist\n",
    "models_dir = \"/kaggle/working/trained_models\"\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "# Full path with filename to save the model\n",
    "model_path = os.path.join(models_dir, \"RetinaNet_model.pth\")\n",
    "\n",
    "# Save the model state dictionary to the specified path\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "print(f\"Model saved at {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zE_YiYUQNmbw"
   },
   "source": [
    "Let´s observe the training evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZsE2UGL3NqH3",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# {   21   }\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "loss_dir = \"/kaggle/working/loss_evo/frcnn\"  # Aquí está la carpeta donde se guardan los CSV\n",
    "output_dir = os.path.join(loss_dir, \"loss_plots\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "csv_files = sorted(\n",
    "    [f for f in os.listdir(loss_dir) if f.startswith(\"loss_epoch_\") and f.endswith(\".csv\")],\n",
    "    key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0])\n",
    ") # Get CSV file\n",
    "\n",
    "all_dfs = []\n",
    "global_step = 0\n",
    "\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(os.path.join(loss_dir, file))\n",
    "    df[\"global_step\"] = df.index + global_step\n",
    "    global_step = df[\"global_step\"].iloc[-1] + 1\n",
    "    all_dfs.append(df)\n",
    "\n",
    "full_df = pd.concat(all_dfs, ignore_index=True) # Full dataframe\n",
    "\n",
    "# Loss components\n",
    "loss_components = [\"loss\", \"classification\", \"bbox_regression\"]\n",
    "\n",
    "titles = {\n",
    "    \"loss\": \"Total Loss\",\n",
    "    \"classification\": \"Classification Loss\",\n",
    "    \"bbox_regression\": \"Bounding Box Regression Loss\"\n",
    "}\n",
    "\n",
    "for loss_name in loss_components:\n",
    "    if loss_name not in full_df.columns:\n",
    "        print(f\"Warning: {loss_name} not found in DataFrame columns\")\n",
    "        continue\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(full_df[\"global_step\"], full_df[loss_name], label=titles[loss_name], linewidth=2)\n",
    "    plt.xlabel(\"Global Step (accumulated batch index)\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Evolution of {titles[loss_name]}\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    output_path = os.path.join(output_dir, f\"{loss_name}_evolution.png\")\n",
    "    plt.savefig(output_path, dpi=300)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_dIzk3PvbSYu",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# {   22   }\n",
    "# Evaluation of the Faster R-CNN modelll\n",
    "import torch\n",
    "\n",
    "# Use GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load pretrained model\n",
    "state_dict=torch.load(\"/kaggle/working/trained_models/RetinaNet_model.pth\")\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# Evaluate the model\n",
    "mAP_map=evaluate(model, dataloader_test, num_classes=n_classes)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7213299,
     "sourceId": 11983006,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
